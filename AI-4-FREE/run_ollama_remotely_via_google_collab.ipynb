{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheRealFREDP3D/AI-4-FREE/blob/main/run_ollama_remotely_via_google_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nTvTOguUuFp",
        "outputId": "22a27841-9cd6-487c-8546-b7b125898e54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YI9Cxw8PyWN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8052daef-dc3d-46ba-e160-9cfb09e83631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n",
            "Ollama is not installed. Installing Ollama...\n",
            "Ollama installation was successful.\n",
            "Ngrok Tunnel URL: https://35d0-34-90-229-192.ngrok-free.app\n",
            "Ollama serve process started.\n"
          ]
        }
      ],
      "source": [
        "# filename: run_ollama_remotely_via_google_collab.ipynb\n",
        "\n",
        "# Install pyngrok\n",
        "!pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import subprocess\n",
        "\n",
        "try:\n",
        "    # Terminate any existing ngrok sessions\n",
        "    ngrok.kill()\n",
        "\n",
        "    # Retrieve your ngrok auth token from the Colab secret storage\n",
        "    auth_token = userdata.get('authtoken')\n",
        "    if not auth_token:\n",
        "        raise ValueError(\"Ngrok auth token not found in Colab secrets.\")\n",
        "\n",
        "    # Set the ngrok authentication token programmatically\n",
        "    ngrok.set_auth_token(auth_token)\n",
        "\n",
        "    # Check if Ollama is installed; if not, install it\n",
        "    try:\n",
        "        ollama_installed = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Ollama is not installed. Installing Ollama...\")\n",
        "\n",
        "        # Install Ollama using the provided command\n",
        "        install_ollama = subprocess.run(\"curl https://ollama.ai/install.sh | sh\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "        # Check if Ollama installation was successful\n",
        "        ollama_installed = subprocess.run([\"ollama\", \"--version\"], capture_output=True, text=True)\n",
        "\n",
        "    if ollama_installed.returncode == 0:\n",
        "        print(\"Ollama installation was successful.\")\n",
        "\n",
        "        # Start an HTTP tunnel on the desired port, e.g., 11434\n",
        "        tunnel = ngrok.connect(11434)\n",
        "        print(\"Ngrok Tunnel URL:\", tunnel.public_url)\n",
        "\n",
        "        # Run Ollama serve\n",
        "        ollama_process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "        print(\"Ollama serve process started.\")\n",
        "    else:\n",
        "        print(\"Ollama installation failed.\")\n",
        "        raise Exception(\"Ollama installation failed.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", str(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !ollama run deepseek-coder-v2:latest\n",
        "# !ollama serve"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k628eB6RUqR",
        "outputId": "c5ec4a73-e540-4286-8928-ff345085cf31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024/08/07 05:31:36 routes.go:1108: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
            "time=2024-08-07T05:31:36.190Z level=INFO source=images.go:781 msg=\"total blobs: 6\"\n",
            "time=2024-08-07T05:31:36.191Z level=INFO source=images.go:788 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-08-07T05:31:36.191Z level=INFO source=routes.go:1155 msg=\"Listening on 127.0.0.1:11434 (version 0.3.3)\"\n",
            "time=2024-08-07T05:31:36.195Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama1985792121/runners\n",
            "time=2024-08-07T05:31:52.831Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx cpu_avx2 cuda_v11 rocm_v60102 cpu]\"\n",
            "time=2024-08-07T05:31:52.831Z level=INFO source=gpu.go:205 msg=\"looking for compatible GPUs\"\n",
            "time=2024-08-07T05:31:53.063Z level=INFO source=types.go:105 msg=\"inference compute\" id=GPU-6cb67893-76d4-a7ab-1b04-d655b4555f18 library=cuda compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mI8PcVSAzRo5"
      },
      "execution_count": 1,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}